[project]
name = "evalbenchs"
version = "0.1.0"
description = "Run LLMs on domain benchmarks via OpenRouter"
readme = "README.md"
requires-python = ">=3.10"
license = "MIT"
authors = [{name = "Evalbenchs"}]
dependencies = [
  "datasets>=2.18.0",
  "gigachat>=0.1.36",
  "openai>=1.0.0",
  "pyyaml>=6.0.1",
  "requests>=2.28.0",
]

[project.scripts]
run-benchmarks = "evalbenchs.cli:main"
inspect-benchmarks = "evalbenchs.inspect:main"
analyze-errors = "evalbenchs.analyze_errors:main"

[tool.setuptools.packages.find]
include = ["evalbenchs", "evalbenchs.*"]

[tool.ruff]
line-length = 100

[tool.ruff.lint]
select = ["E", "F", "I", "UP"]
